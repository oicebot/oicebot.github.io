---
layout: post
title: "Benchmarking TensorFlow Lite on the New Raspberry Pi 4, Model B"
tags: udacity translate python Machine-Learning
author: Alasdair Allan
from: https://blog.hackster.io/benchmarking-tensorflow-lite-on-the-new-raspberry-pi-4-model-b-3fd859d05b98
excerpt: "How much faster is the new Raspberry Pi? Itâ€™s a lot faster."
thumb: "/img/20190730/thumb.png"
---

å‰ä¸ä¹…ï¼Œæ ‘è“æ´¾åŸºé‡‘ä¼šåˆå‘å¸ƒäº†æœ€æ–°çš„ Raspberry Pi 4 ä»£å•ç‰‡æœºç”µè„‘ï¼Œä¸ä½†å¤§å¹…æå‡äº†èŠ¯ç‰‡è¿ç®—èƒ½åŠ›ï¼Œå¯é€‰å†…å­˜ä¹Ÿå¢åŠ åˆ°äº†æœ€å¤š 4GBï¼Œè®©è¿™å¼ å°å¡ç‰‡æ‹¥æœ‰äº†ç±»ä¼¼ PC çº§åˆ«çš„æ€§èƒ½ã€‚è€Œ 35ï½55 ç¾å…ƒçš„è¶…ä½ä»·æ ¼ï¼ˆå›½å†…ä»£è´­çš„é›¶å”®ä»·ä¸€èˆ¬åœ¨ 200 ï½ 400 å…ƒäººæ°‘å¸å·¦å³ï¼Œä¹Ÿç®—èƒ½æ¥å—å¾—äº†å•¦ï¼‰ï¼Œè®©æ ‘è“æ´¾ä¸€ç›´ä»¥æ¥éƒ½æ˜¯å­¦æ ¡å’Œè®¡ç®—æœºçˆ±å¥½è€…æ‰‹ä¸­çš„â€œç¥å™¨â€ã€‚

Raspberry Pi 4B å‹ç¡¬ä»¶å‚æ•°ï¼š

* 1.5GHz å››æ ¸ 64 ä½ ARM Cortex-A72 èŠ¯ç‰‡
* å¯é€‰ 1 / 2 / 4GB LPDDR4 SDRAM å†…å­˜
* å…¨åŒå·¥åƒå…†ä»¥å¤ªç½‘æ¥å£
* æ¿è½½åŒé¢‘802.11acæ— çº¿ç½‘ç»œ
* æ¿è½½è“ç‰™5.0
* å¸¦æœ‰ä¸¤ä¸ª USB 3.0 å’Œä¸¤ä¸ª USB 2.0 æ¥å£
* 2 ä¸ª micro HDMI è¾“å‡ºï¼Œæ”¯æŒåŒæ—¶é©±åŠ¨åŒæ˜¾ç¤ºå™¨ï¼Œåˆ†è¾¨ç‡é«˜è¾¾ 4K
* VideoCore VI æ˜¾ç¤ºèŠ¯ç‰‡ï¼Œæ”¯æŒ OpenGL ES 3.x.
* æ”¯æŒ HEVC è§†é¢‘ 4Kp60 ç¡¬è§£ç 
* USB Type-C ä¾›ç”µæ¥å£

ä¸ä»…å¦‚æ­¤ï¼Œæœ€è¿‘è¿˜æœ‰å¤§ä½¬æˆåŠŸåœ¨æ ‘è“æ´¾ 4 ä¸Šè·‘èµ·äº† TensorFlowï¼Œæäº†ä¸€æŠŠæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½è¯„æµ‹ï¼ŒçœŸçš„è¿™ä¹ˆç¥å—ï¼

è®©æˆ‘ä»¬ä¸€èµ·çœ‹çœ‹å§ï¼

## å¤ªé•¿ä¸çœ‹ç‰ˆ

Using TensorFlow Lite we see a considerable speed increase when compared with the original results from our previous benchmarks using full TensorFlow.
We see between a Ã—3 and Ã—4 increase in inferencing speed between our original TensorFlow benchmark, and the new results using TensorFlow Lite. 

1. ç”¨ä¸Šäº†æœ€æ–°çš„ TensorFlow Lite ä¹‹åï¼Œè·‘åŒä¸€ä¸ªæ•°æ®é›†çš„é€Ÿåº¦è¾¾åˆ°äº†ä¹‹å‰ç”¨ TensorFlow æ—¶çš„ 3 è‡³ 4 å€ã€‚

This decrease in inferencing time brings the Raspberry Pi 4 directly into competition with the NVIDIA Jetson Nano.

2. æ ‘è“æ´¾ 4B å¤„ç†æœºå™¨å­¦ä¹ ä»»åŠ¡çš„ç®—åŠ›è¶…è¿‡æ ‘è“æ´¾ 3B+ çš„4å€ï¼Œå·²ç»èƒ½å’Œ NVIDIA Jetson Nano æœ‰çš„ä¸€æ‹¼äº†ã€‚

<img src="/img/20190730/001.jpg"><br><small>
æ–°æ ‘è“æ´¾ 4b çš„æœºå™¨å­¦ä¹ ä»»åŠ¡è·‘åˆ†ç»“æœã€‚å•ä½ï¼šæ¯«ç§’</small>

Benchmarking results in milli-seconds for MobileNet v1 SSD 0.75 depth model and the MobileNet v2 SSD model, both models trained using the Common Objects in Context (COCO) dataset with an input size of 300Ã—300, for the new Raspberry Pi 4, Model B, running Tensor Flow (blue) and TensorFlow Lite (green).

3. åœ¨æ–°æ ‘è“æ´¾ 4b ä¸Šåˆ†åˆ«ä½¿ç”¨ MobileNet v1 SSD 0.75 æ·±åº¦æ¨¡å‹ï¼Œä»¥åŠ MobileNet v2 SSD æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œéƒ½ä½¿ç”¨äº† Common Objects in Context (COCO) æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¾“å…¥å›¾åƒåˆ†è¾¨ç‡éƒ½æ˜¯ 300x300ï¼Œä½¿ç”¨ TensorFlow æ—¶è¿ç®—æ—¶é—´åˆ†åˆ«ä¸º 263.9 æ¯«ç§’å’Œ 483.5 æ¯«ç§’ï¼Œè€Œä½¿ç”¨ TensorFlow Lite æ—¶çš„è¿ç®—æ—¶é—´ä¸º 82.7 æ¯«ç§’å’Œ 122.6 æ¯«ç§’ã€‚

## ç¬¬ä¸€éƒ¨åˆ† è·‘åˆ†æµ‹è¯•ç»†èŠ‚

### è®¾å¤‡ã€æ¨¡å‹å’Œæ•°æ®é›†

Benchmarking was done using both TensorFlow and TensorFlow Lite on a Raspberry Pi 3, Model B+, and on the 4GB version of the Raspberry Pi 4, Model B. Inferencing was carried out with the MobileNet v2 SSD and MobileNet v1 0.75 depth SSD models, both models trained on the Common Objects in Context (COCO) dataset, converted to TensorFlow Lite.

æˆ‘ä»¬åœ¨æ ‘è“æ´¾ 3b+ã€æ ‘è“æ´¾ 4Bï¼ˆ4Gå†…å­˜ç‰ˆï¼‰ä»¥åŠä¸€äº›å…¶ä»–è®¾å¤‡ä¸Šéƒ½è¿›è¡Œäº†æµ‹è¯•ï¼Œæµ‹è¯•çš„æ¨¡å‹å‡ä¸º MobileNet v2 SSD ä»¥åŠ MobileNet v1 0.75 æ·±åº¦ SSD æ¨¡å‹ï¼Œä¸”éƒ½ä½¿ç”¨ COCO æ•°æ®é›†è¿›è¡Œäº†è®­ç»ƒã€‚

These results can now be compared to our previously obtained benchmark results on the following platforms; the Coral Dev Board, the NVIDIA Jetson Nano, the Coral USB Accelerator with a Raspberry Pi, the original Movidus Neural Compute Stick with a Raspberry Pi, and the second generation Intel Neural Compute Stick 2 again with a Raspberry Pi. Comparison was also made with the Xnor.ai AI2GO platform using their proprietary binary convolution network.

æˆ‘ä»¬ä½¿ç”¨çš„å…¶ä»–è®¾å¤‡åŒ…æ‹¬ Coral å¼€å‘æ¿ã€NVIDIA Jetson Nanoï¼Œåˆ†åˆ«åŠ æŒ‚äº† Coral USB åŠ é€Ÿå™¨ã€åˆä»£ Movidus ç¥ç»ç½‘ç»œè®¡ç®—æ£’ã€äºŒä»£è‹±ç‰¹å°”ç¥ç»ç½‘ç»œè®¡ç®—æ£’çš„æ ‘è“æ´¾ï¼Œä»¥åŠä¸€å° MacBook Proã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜åŠ å…¥äº†åœ¨æ ‘è“æ´¾ä¸Šè¿è¡Œ Xnor.ai çš„ AI2GO å¹³å°ï¼Œä½¿ç”¨çš„æ˜¯ Xnor çš„ç§æœ‰å·ç§¯ç½‘ç»œç¨‹åºã€‚

> â„¹ï¸ Information The Raspberry Pi 3, Model B+, has no USB 3 support, so no results are available for the Coral USB Accelerator using USB on the Raspberry Pi 3. While results for both generations of the Movidius-based Compute Stick, the Movidus Neural Compute Stick and the Intel Neural Compute Stick 2 are not available on the Raspberry Pi 4, Model B, as the Intel OpenVINO framework does not yet work with Python 3.7. You should not expect official support for the Intel Neural Compute Stick on the Raspberry Pi 4 in the near term.

> â„¹ï¸ é™„æ³¨ï¼šæ ‘è“æ´¾ 3B+ å‹æ²¡æœ‰ USB 3 æ¥å£ï¼Œæ‰€ä»¥æ— æ³•ä½¿ç”¨ USB 3 ç‰ˆæœ¬çš„ Coral USB åŠ é€Ÿå™¨ã€‚ç”±äº Intel OpenVINO ä¸æ”¯æŒ Python 3.7ï¼Œæ‰€ä»¥åˆä»£ Movidus ç¥ç»ç½‘ç»œè®¡ç®—æ£’å’ŒäºŒä»£è‹±ç‰¹å°”ç¥ç»ç½‘ç»œè®¡ç®—æ£’åœ¨æ ‘è“æ´¾ 4 ä¸Šè¿˜æ— æ³•æ­£å¸¸å·¥ä½œã€‚ä½†éšç€æ–°è®¾å¤‡çš„æ™®åŠï¼Œè¿‘æœŸå†…å®˜æ–¹å¯èƒ½å°±ä¼šæ¨å‡ºé’ˆå¯¹æ ‘è“æ´¾ 4 çš„é€‚é…äº†ã€‚

åœ¨çº¯é æ ‘è“æ´¾ç®—åŠ›çš„ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬è¿˜è¿›è¡Œäº† TensorFlow å’Œ TensorFlow Liteï¼ˆæ¨¡å‹ç»è¿‡è½¬æ¢ï¼‰çš„å¯¹æ¯”æµ‹è¯•ã€‚

A single 3888Ã—2916 pixel test image was used containing two recognisable objects in the frame, a bananağŸŒ and an appleğŸ. The image was resized down to 300Ã—300 pixels before presenting it to the model, and each model was run 10,000 times before an average inferencing time was taken. The first inferencing run, which takes longer due to loading overheads, was discarded.

æœºå™¨å­¦ä¹ ä»»åŠ¡æ–¹é¢ï¼Œæˆ‘å‡†å¤‡äº†ä¸€å¼ åˆ†è¾¨ç‡ä¸º 3888x2916 çš„å¾…è¯†åˆ«å›¾ç‰‡ï¼Œå›¾ç‰‡ä¸­åŒ…å«ä¸¤ä¸ªå¯è¯†åˆ«çš„å¯¹è±¡ï¼šä¸€ä¸ªé¦™è•‰ğŸŒï¼Œä¸€ä¸ªè‹¹æœğŸã€‚åœ¨å–‚ç»™æ¨¡å‹ä¹‹å‰ï¼Œå›¾ç‰‡å°†ä¼šè¢«ç¼©å°åˆ° 300x300 åƒç´ ï¼Œæ¯ä¸ªæ¨¡å‹å°†ä¼šæ‰§è¡Œä¸€ä¸‡æ¬¡ï¼ŒæŠ›å¼ƒç¬¬ä¸€æ¬¡çš„å¤„ç†ç»“æœï¼ˆå¯èƒ½å­˜åœ¨å› ä¸ºè½½å…¥ç“¶é¢ˆé€ æˆçš„å»¶è¿Ÿï¼‰ï¼Œå°†å‰©ä¸‹çš„å¤„ç†ç»“æœå–å¹³å‡é€Ÿåº¦ã€‚

<img src="/img/20190730/002.jpg"><br><small>
ç¨‹åºè¦è¯†åˆ«çš„å°±æ˜¯è¿™æ ·å¼ å›¾</small>

### è¯¦ç»†æ•°æ®

<img src="/img/20190730/003.jpg"><br><small>
Benchmarking results in milli-seconds for MobileNet v1 SSD 0.75 depth model and the MobileNet v2 SSD model, both trained using the Common Objects in Context (COCO) dataset with an input size of 300Ã—300. Results for the Xnor.ai AI2GO platform are using their proprietary binary convolution network.
æµ‹è¯•ç»“æœï¼Œå•ä½ï¼šæ¯«ç§’ã€‚å…¶ä¸­åœ¨ Xnor.ai çš„ AI2GO å¹³å°ä¸Šè¿è¡Œçš„ç¨‹åºä½¿ç”¨çš„æ˜¯ä»–ä»¬ç§æœ‰çš„å·ç§¯ç½‘ç»œç¨‹åºã€‚
</small>

> âš ï¸Warning As per our previous results with the Raspberry Pi 4 the addition of a small fan, driven from the Raspberry Piâ€™s own GPIO headers, was need to keep the CPU temperature stable and prevent thermal throttling of the CPU.
> âš ï¸ æ³¨æ„ï¼šæ ¹æ®æˆ‘ä»¬ä¹‹å‰å¯¹æ ‘è“æ´¾ 4 çš„è¯„æµ‹ç»“æœï¼Œä½ éœ€è¦åœ¨æ¿å­ä¸Šæ·»åŠ ä¸€ä¸ªç”±æ ‘è“æ´¾ GPIO å£é©±åŠ¨çš„æ•£çƒ­é£æ‰‡ï¼Œä»¥ä¾¿ä¿æŒ CPU çš„æ¸©åº¦ç¨³å®šï¼Œé¿å…å› ä¸ºé«˜æ¸©é€ æˆ CPU ä¿æŠ¤æ€§é™é€Ÿã€‚

Our initial TensorFlow results on the new Raspberry Pi 4 showed a Ã—2 increase in performance. This is roughly in line with expectations as with twice the NEON capacity more than the Raspberry Pi 3, we would expect this order of speedup in performance for well-written NEON kernels.

ä¸€å¼€å§‹ï¼Œåœ¨ TensorFlow æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬å‘ç°æ ‘è“æ´¾ 4 çš„å¤„ç†é€Ÿåº¦æ¯”ä¸Šä»£æå‡äº†å¤§çº¦ä¸€å€ï¼Œè¿™å·®ä¸å¤šç®—æ˜¯å› ä¸ºæ–°çš„ ARM Cortex-A72 å¤„ç†å™¨å¯¹ NEON æŒ‡ä»¤é›†çš„å¤„ç†å®¹é‡æ¯”ä¸Šä»£å¤šäº†å°†è¿‘ä¸€å€ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœèƒ½ç”¨ä¸Šæ•ˆç‡æ›´é«˜çš„ NEON å†…æ ¸ä»£ç ï¼Œè¿™ä¸ªé€Ÿåº¦è¿˜æœ‰æå‡çš„ç©ºé—´ã€‚

However we see a significantly larger speed increase with TensorFlow Lite, with a Ã—3 to Ã—4 increase in inferencing speeds between our TensorFlow benchmark, and the new results using TensorFlow Lite. This result is much larger than we saw when a similar comparison was made with the Raspberry Pi 3, where we saw only a Ã—2 increase in performance between the two packages. We are therefore seeing almost double the expected speed gain by using TensorFlow Lite over TensorFlow on the Raspberry Pi 4.

æ¥ç€ï¼Œæˆ‘ä»¬å‘ç°åœ¨ä½¿ç”¨ TensorFlow Lite çš„æ—¶å€™ï¼Œæ•´ä½“é€Ÿåº¦æœ‰äº†ç›¸å½“æ˜¾è‘—çš„æå‡ï¼Œæ€»é€Ÿåº¦è¾¾åˆ°äº† TensorFlow æµ‹è¯•çš„ 3ï½4 å€ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨æ ‘è“æ´¾ 3 ä¸Šï¼ŒTensorFlow Lite çš„æå‡åˆ™ç›¸å¯¹æœ‰é™ï¼Œåªèƒ½è¾¾åˆ°åŸæ¥çš„ 2 å€ä¸Šä¸‹ã€‚

<img src="/img/20190730/004.jpg"><br><small>
å„è®¾å¤‡è¿ç®—æ—¶é—´æ¨ªå‘å¯¹æ¯”ã€‚å•ä½ï¼šæ¯«ç§’</small>

Inferencing time in milli-seconds for the for MobileNet v1 SSD 0.75 depth model (left hand bars) and the MobileNet v2 SSD model (right hand bars), both trained using the Common Objects in Context (COCO) dataset with an input size of 300Ã—300. The (single) bars for the Xnor AI2GO platform use their proprietary binary weight model. All measurements on the Raspberry Pi 3, Model B+, are shown in yellow, measurements on the Raspberry Pi 4, Model B, are shown in red. Other stand-alone platforms that are not dependent on the Raspberry Pi are shown in green.

ä¸Šå›¾æ˜¯å„è®¾å¤‡è¿ç®—æ—¶é—´çš„æ¨ªå‘å¯¹æ¯”ã€‚æ¯ä¸€ä¸ªè®¾å¤‡æœ‰ä¸¤ç»„æ•°æ®ï¼Œå·¦ä¾§çš„æ˜¯ç”¨ MobileNet v1 SSD 0.75 æ·±åº¦æ¨¡å‹ï¼Œå³ä¾§çš„æ˜¯ MobileNet v2 SSD æ¨¡å‹ã€‚Xnor AI2GO å¹³å°çš„ä¸¤ä¸ªè®¾å¤‡ï¼ˆæ ‘è“æ´¾3/4ï¼‰éƒ½åªä½¿ç”¨ Xnor ç§æœ‰çš„æƒé‡æ¨¡å‹ã€‚Raspberry Pi 3B+ çš„æ‰€æœ‰æµ‹è¯•ç»“æœå‡ä»¥é»„è‰²æ˜¾ç¤ºï¼ŒRaspberry Pi 4B ä¸Šçš„æµ‹è¯•ç»“æœä»¥çº¢è‰²æ˜¾ç¤ºã€‚å…¶ä»–ä¸ä¾èµ–äº Raspberry Pi çš„ç‹¬ç«‹å¹³å°ä»¥ç»¿è‰²æ˜¾ç¤ºã€‚

This decrease in inferencing time brings the Raspberry Pi 4 directly into competition with both the NVIDIA Jetson Nano and the Movidius-based hardware from Intel.

æ‹œ TensorFlow Lite æ‰€èµï¼Œæ ‘è“æ´¾ 4 çš„è¿ç®—æ—¶é—´å·²ç»å‡å°‘åˆ°èƒ½æ­£é¢åˆš NVIDIA Jetson Nano å’Œè‹±ç‰¹å°” Movidius ç³»åˆ—ç¡¬ä»¶çš„ç¨‹åº¦äº†ã€‚

> âš ï¸Warning It is probable that the Movidius Neural Compute Stick and the Intel Neural Compute Stick 2 will show better performance when connected to the Raspberry Pi 4 using USB 3 rather than USB 2. However until the OpenVINO framework supports Python 3.7 it is impossible to know for certain. Right now the Movidius-based hardware from Intel is **not usable** with the Rapsberry Pi 4.

> âš ï¸ æ³¨æ„ï¼šMovidius ç¥ç»ç½‘ç»œè®¡ç®—æ£’å’Œè‹±ç‰¹å°”ç¥ç»ç½‘ç»œè®¡ç®—æ£’ 2 ä»£å› ä¸ºæ ‘è“æ´¾ 3 æ²¡æœ‰ USB 3 çš„æ¥å£ï¼Œæ‰€ä»¥åªèƒ½åœ¨ USB 2 ä¸‹å·¥ä½œï¼Œæ‰€ä»¥é€Ÿåº¦å—åˆ°äº†ä¸€å®šçš„é™åˆ¶ã€‚ç„¶è€Œï¼Œ<span class="hl">ç›®å‰è¿™ä¸¤ä¸ªè®¾å¤‡æ— æ³•åœ¨æ ‘è“æ´¾ 4 ä¸Šæ­£å¸¸å·¥ä½œ</span>ï¼Œæ‰€ä»¥å¯¹åº”çš„æµ‹è¯•æ— æ³•è¿›è¡Œã€‚ä¹Ÿè®¸è¿™è¦ç­‰ OpenVINO æ¡†æ¶æ”¯æŒ Python 3.7 ä¹‹åäº†å§ã€‚

If you were looking at purchasing the NVIDIA Jetson Nano to use for machine learning, there now seems no reason to do so as the Raspberry Pi 4 performs at a similar level, but for half the cost.

å¦‚æœä½ æ›¾ç»å‡†å¤‡è´­ä¹° NVIDIA Jetson Nano æ¥è¿›è¡Œæœºå™¨å­¦ä¹ å·¥ä½œï¼Œæˆ‘ä¸ªäººè§‰å¾—ä½ ä¸å¦¨çœ‹çœ‹ä»·æ ¼åªæœ‰å®ƒä¸€åŠçš„æ ‘è“æ´¾ 4 ğŸ˜Šã€‚

### ç»“æœæ±‡æ€»

The performance increase seen with the new Raspberry Pi 4 makes it a very competitive platform for machine learning inferencing at the edge. The increase in inferencing performance we see with TensorFlow Lite on the Raspberry Pi 4 puts it directly into competition with the NVIDIA Jetson Nano and the Intel Neural Compute Stick 2.

Priced at $35 for the 1GB version, and $55 for the 4GB version, the new Raspberry Pi 4 is significantly cheaper than both the NVIDIA Jetson Nano, and the Intel Neural Compute Stick 2, both of which cost $99. Especially considering that, for the Compute Stick, this cost is in addition to the cost of the Raspberry Pi itself which therefore comes to a total of $134.

While the Coral Dev Board from Google is still the â€˜best in classâ€™ board, the addition on USB 3 to the Raspberry Pi 4 means that it is now also price competitive with the Dev Board. Priced at $35 the 1GB version of the new Raspberry Pi 4 is significantly cheaper than the $149 Coral Dev Board. Adding an additional $74.99 for the Coral USB Accelerator to the price of the Raspberry Pi means that you can outperform the previous â€˜best in classâ€™ board for a cost of $109.99. Thatâ€™s a saving of $39.01 over the cost of the Coral Dev Board, for better performance.

## ç¬¬äºŒéƒ¨åˆ† æµ‹è¯•æ–¹æ³•ä¸ä»£ç 

### åœ¨æ ‘è“æ´¾ä¸Šå®‰è£… TensorFlow Lite

Installing TensorFlow on the Raspberry Pi used to be a difficult process, however towards the middle of last year everything became a lot easier. Fortunately, thanks to the community, installing TensorFlow Lite isnâ€™t that much harder. We arenâ€™t going to have to resort to building it from source.

<img src="/img/20190730/005.jpg"><br><small>
The new Raspberry Pi 4.</small>

Go ahead and download the latest release of Raspbian Lite and set up your Raspberry Pi. Unless youâ€™re using wired networking, or have a display and keyboard attached to the Raspberry Pi, at a minimum youâ€™ll need to put the Raspberry Pi on to your wireless network, and enable SSH.

Once youâ€™ve set up your Raspberry Pi go ahead and power it on, and then open up a Terminal window on your laptop and SSH into the Raspberry Pi.

```
% ssh pi@raspberrypi.local
```

Fortunately while the official TensorFlow binary distribution does not include a build of TensorFlow Lite, there is an unofficial distribution which does, and that means we donâ€™t have to resort to building and install from source.

Once youâ€™re logged into your Raspberry Pi go ahead and update and install our build tools, before going ahead and install TensorFlow Lite.

```
$ sudo apt-get update
$ sudo apt-get install build-essential
$ sudo apt-get install git
```

> â„¹ï¸ Information If youâ€™re working on an existing installation, and you already have the official version of TensorFlow installed, you should make sure you have uninstalled it first, by doing `sudo pip3 uninstall tensorflow`.

While there isnâ€™t yet a build of TensorFlow Lite specifically for Python 3.7, we can make use of one of the Python 3.5 builds. However, youâ€™ll need to make some tweaks before installation.

```
$ sudo apt-get install libatlas-base-dev
$ sudo apt-get install python3-pip
$ git clone https://github.com/PINTO0309/Tensorflow-bin.git
$ cd Tensorflow-bin
$ mv tensorflow-1.14.0-cp35-cp35m-linux_armv7l.whl tensorflow-1.14.0-cp37-cp37m-linux_armv7l.whl
$ pip3 install --upgrade setuptools
$ pip3 install tensorflow-1.14.0-cp37-cp37m-linux_armv7l.whl
```

Itâ€™ll take some time to install. So you might want to take a break and get some coffee. Once it has finished installing you can test the installation as follows.

```
$ python3 -c "import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))"
```

> âš ï¸Warning You will receive â€˜Runtime Warningsâ€™ when you `import tensorflow`. These arenâ€™t a concern and just indicate that the wheel was built under Python 3.5 and youâ€™re using it with Python 3.7. You can safely ignore the warnings.

Now TensorFlow has been successfully installed we need to install OpenCV, the Pillow fork of the Python Imaging Library (PIL) and the NumPy library.

```
$ sudo apt-get install python3-opencv
$ pip3 install Pillow
$ pip3 install numpy
```

We should now be ready to run our benchmarking scripts.

### æµ‹è¯•ä»£ç åŠèµ„æ–™ä¸‹è½½


The code from our previous benchmarks was reused unchanged.

> é“¾æ¥: https://pan.baidu.com/s/1DzUpkF89bYGccMXExXJMNQ æå–ç : e7fr 


## ç»“è¯­

Comparing these platforms on an even footing continues to be difficult. But it is clear that the new Raspberry Pi 4 is a solid platform for machine learning inferencing at the edge.

> _ï¼ˆæœ¬æ–‡å·²æŠ•ç¨¿ç»™ã€Œ[ä¼˜è¾¾å­¦åŸ](https://cn.udacity.com)ã€ã€‚ åŸä½œï¼š [{{ page.author }}]({{ page.from }}) ï¼Œè¯‘è€…ï¼šæ¬§å‰ƒ è½¬è½½è¯·ä¿ç•™æ­¤ä¿¡æ¯ï¼‰_